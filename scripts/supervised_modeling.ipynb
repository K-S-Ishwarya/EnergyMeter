{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LQ classify_device.ipynb","version":"0.3.2","provenance":[{"file_id":"19k7VSPlcojOQ8P35WJNSibwMWfwXLHsQ","timestamp":1551460715898}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"1odMcPHm0py5","colab_type":"text"},"cell_type":"markdown","source":["------------------------------------------------------------------\n","# IMPORT LIBRARIES\n","------------------------------------------------------------------"]},{"metadata":{"id":"-CTZEReucZVQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# import standard libraries\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","import keras\n","# tf.enable_eager_execution()\n","\n","#import plotting libraries\n","import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n","warnings.filterwarnings(\"ignore\")\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# import file handler libraries\n","!pip install -U -q PyDrive\n","import sys, os, shutil, zipfile,glob\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","from sklearn.metrics import confusion_matrix, accuracy_score,roc_curve, auc,roc_auc_score,classification_report\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ih43MajiVC4T","colab_type":"code","colab":{}},"cell_type":"code","source":["#clear files\n","!rm -rf /content/training\n","!rm -rf /content/training_data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hFJlQN3xFfIl","colab_type":"code","outputId":"7273acc1-cdeb-4402-c6f6-089c2a0e1bf1","executionInfo":{"status":"error","timestamp":1552581106521,"user_tz":420,"elapsed":7573,"user":{"displayName":"Louis Quicksell","photoUrl":"","userId":"02987867429841815458"}},"colab":{"base_uri":"https://localhost:8080/","height":337}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)\n","!ls '/content/gdrive/My Drive/Models'"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-6053a74d34fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ls '/content/gdrive/My Drive/Models'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"]}]},{"metadata":{"id":"Si0Tumih0uLK","colab_type":"text"},"cell_type":"markdown","source":["------------------------------------------------------------------\n","# LINK DATA TO INSTANCE\n","------------------------------------------------------------------"]},{"metadata":{"id":"iPN0aqfJmPNw","colab_type":"code","outputId":"aafbc03e-6bfb-4f32-833c-d6bc81ac34a2","executionInfo":{"status":"ok","timestamp":1552581260936,"user_tz":420,"elapsed":10066,"user":{"displayName":"Louis Quicksell","photoUrl":"","userId":"02987867429841815458"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["# zip_id = '1LvGrbtylxRGegXc4W9ZOnosqmV1J855V' #FILE ID CREATED FROM SHARING URL (ID=....)\n","zip_id = '1jH_aAu-rNBDGUdoag8duHjN2xYHvbraE' #FILE ID CREATED FROM SHARING URL (ID=....)\n","\n","auth.authenticate_user() # 1. Authenticate and create the PyDrive client.\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","if not os.path.exists('training'): #create directory \n","    os.makedirs('training')\n","    print('made directory')\n","\n","# DOWNLOAD ZIP\n","print (\"Downloading zip file\")\n","myzip = drive.CreateFile({'id': zip_id})\n","myzip.GetContentFile('training 2.zip')\n","\n","# UNZIP ZIP\n","print (\"Uncompressing zip file\")\n","zip_ref = zipfile.ZipFile('training 2.zip', 'r') #file to be extracted\n","zip_ref.extractall('./training') #where the files are extracted\n","zip_ref.close()\n","\n","training_path = '/content/training/training/'\n","print('finished unzipping, contents:\\n',os.listdir(training_path))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["made directory\n","Downloading zip file\n","Uncompressing zip file\n","finished unzipping, contents:\n"," ['heatpad_I_84.csv', 'hairdryer-high_ricky_81.csv', 'inductioncooktop_ricky_17.csv', 'kettle_ricky_58.csv', 'laptop_will_179.csv', 'hairdryer-low_ricky_15.csv', 'hairdryer-high_ricky_78.csv', 'inductioncooktop_ricky_74.csv', 'laptop_ricky_13.csv', 'hairdryer-high_ricky_30.csv', 'kettle_ricky_23.csv', 'inductioncooktop_ricky_3.csv', 'kettle_ricky_61.csv', 'hairdryer-high_ricky_29.csv', 'laptop_will_112.csv', 'heatpad_I_9.csv', 'laptop_will_90.csv', 'hairdryer-high_ricky_37.csv', 'heatpad_I_49.csv', 'inductioncooktop_ricky_12.csv', 'hairdryer-high_ricky_41.csv', 'heatpad_I_51.csv', 'laptop_will_3.csv', 'laptop_ricky_2.csv', 'laptop_will_95.csv', 'kettle_ricky_97.csv', 'inductioncooktop_ricky_53.csv', 'laptop_will_174.csv', 'inductioncooktop_ricky_62.csv', 'kettle_ricky_35.csv', 'inductioncooktop_ricky_14.csv', 'hairdryer-low_ricky_13.csv', 'heatpad_II_7.csv', 'heatpad_I_13.csv', 'laptop_will_83.csv', 'laptop_will_5.csv', 'laptop_will_94.csv', 'inductioncooktop_ricky_32.csv', 'hairdryer-high_ricky_89.csv', 'inductioncooktop_ricky_76.csv', 'heatpad_II_18.csv', 'laptop_will_7.csv', 'kettle_ricky_86.csv', 'hairdryer-high_ricky_19.csv', 'inductioncooktop_ricky_92.csv', 'laptop_will_117.csv', 'hairdryer-high_ricky_60.csv', 'laptop_ricky_12.csv', 'laptop_will_12.csv', 'heatpad_I_25.csv', 'kettle_ricky_28.csv', 'kettle_ricky_93.csv', 'laptop_will_45.csv', 'heatpad_I_16.csv', 'laptop_will_77.csv', 'kettle_ricky_72.csv', 'laptop_ricky_8.csv', 'LEDpicture_ricky_4.csv', 'laptop_will_21.csv', 'heatpad_III_10.csv', 'hairdryer-high_ricky_57.csv', 'inductioncooktop_ricky_50.csv', 'heatpad_I_95.csv', 'kettle_ricky_88.csv', 'cell_ricky_18.csv', 'hairdryer-low_ricky_1.csv', 'LEDpicture_ricky_1.csv', 'laptop_will_168.csv', 'hairdryer-high_ricky_88.csv', 'inductioncooktop_ricky_96.csv', 'heatpad_II_8.csv', 'kettle_ricky_43.csv', 'inductioncooktop_ricky_81.csv', 'kettle_ricky_56.csv', 'laptop_will_114.csv', 'heatpad_I_55.csv', 'heatpad_I_29.csv', 'hairdryer-high_ricky_36.csv', 'cell_ricky_9.csv', 'laptop_will_195.csv', 'heatpad_I_58.csv', 'heatpad_I_4.csv', 'heatpad_I_38.csv', 'LEDpicture_ricky_9.csv', 'laptop_ricky_20.csv', 'hairdryer-high_ricky_85.csv', 'laptop_will_19.csv', 'inductioncooktop_ricky_64.csv', 'cell_ricky_7.csv', 'laptop_will_199.csv', 'laptop_will_38.csv', 'heatpad_I_31.csv', 'laptop_will_103.csv', 'laptop_will_115.csv', 'laptop_will_74.csv', 'inductioncooktop_ricky_30.csv', 'heatpad_II_16.csv', 'inductioncooktop_ricky_37.csv', 'hairdryer-high_ricky_54.csv', 'laptop_will_194.csv', 'heatpad_III_13.csv', 'laptop_will_142.csv', 'laptop_will_16.csv', 'laptop_will_52.csv', 'heatpad_I_74.csv', 'kettle_ricky_54.csv', 'kettle_ricky_49.csv', 'laptop_will_183.csv', 'hairdryer-high_ricky_24.csv', 'inductioncooktop_ricky_63.csv', 'inductioncooktop_ricky_23.csv', 'laptop_will_132.csv', 'kettle_ricky_63.csv', 'laptop_will_130.csv', 'hairdryer-low_ricky_6.csv', 'laptop_will_86.csv', 'laptop_will_41.csv', 'inductioncooktop_ricky_36.csv', 'laptop_will_133.csv', 'laptop_will_153.csv', 'laptop_will_154.csv', 'hairdryer-high_ricky_32.csv', 'cell_ricky_19.csv', 'laptop_will_129.csv', 'heatpad_III_6.csv', 'kettle_ricky_5.csv', 'laptop_will_65.csv', 'heatpad_III_20.csv', 'laptop_will_185.csv', 'kettle_ricky_84.csv', 'heatpad_III_16.csv', 'kettle_ricky_71.csv', 'hairdryer-high_ricky_49.csv', 'hairdryer-low_ricky_16.csv', 'LEDpicture_ricky_13.csv', 'inductioncooktop_ricky_18.csv', 'inductioncooktop_ricky_83.csv', 'kettle_ricky_20.csv', 'laptop_ricky_11.csv', 'laptop_ricky_19.csv', 'hairdryer-high_ricky_56.csv', 'inductioncooktop_ricky_72.csv', 'cell_ricky_17.csv', 'heatpad_I_36.csv', 'laptop_will_33.csv', 'laptop_will_42.csv', 'laptop_will_97.csv', 'inductioncooktop_ricky_99.csv', 'heatpad_I_1.csv', 'heatpad_III_19.csv', 'inductioncooktop_ricky_46.csv', 'LEDpicture_ricky_12.csv', 'laptop_will_48.csv', 'laptop_will_61.csv', 'cell_ricky_3.csv', 'cell_ricky_8.csv', 'laptop_will_56.csv', 'LEDpicture_ricky_11.csv', 'laptop_will_121.csv', 'hairdryer-high_ricky_69.csv', 'heatpad_I_85.csv', 'laptop_will_111.csv', 'kettle_ricky_3.csv', 'laptop_will_139.csv', 'heatpad_I_91.csv', 'laptop_will_20.csv', 'inductioncooktop_ricky_39.csv', 'laptop_ricky_18.csv', 'inductioncooktop_ricky_68.csv', 'hairdryer-high_ricky_62.csv', 'heatpad_I_28.csv', 'hairdryer-high_ricky_99.csv', 'laptop_will_134.csv', 'heatpad_III_2.csv', 'heatpad_I_18.csv', 'heatpad_I_57.csv', 'heatpad_II_4.csv', 'hairdryer_ricky_20.csv', 'laptop_will_119.csv', 'laptop_will_80.csv', 'kettle_ricky_36.csv', 'laptop_will_18.csv', 'laptop_will_53.csv', 'LEDpicture_ricky_7.csv', 'laptop_will_96.csv', 'hairdryer-high_ricky_25.csv', 'laptop_will_125.csv', 'laptop_will_164.csv', 'laptop_will_124.csv', 'heatpad_III_9.csv', 'laptop_will_141.csv', 'laptop_will_197.csv', 'heatpad_I_99.csv', 'hairdryer-high_ricky_67.csv', 'inductioncooktop_ricky_42.csv', 'hairdryer-high_ricky_10.csv', 'heatpad_I_15.csv', 'laptop_will_173.csv', 'laptop_will_107.csv', 'inductioncooktop_ricky_60.csv', 'hairdryer-high_ricky_16.csv', 'laptop_will_28.csv', 'inductioncooktop_ricky_66.csv', 'laptop_will_180.csv', 'kettle_ricky_57.csv', 'hairdryer-high_ricky_7.csv', 'hairdryer-high_ricky_80.csv', 'inductioncooktop_ricky_55.csv', 'laptop_will_98.csv', 'laptop_will_163.csv', 'laptop_will_198.csv', 'laptop_will_169.csv', 'laptop_will_104.csv', 'laptop_will_17.csv', 'LEDpicture_ricky_3.csv', 'inductioncooktop_ricky_16.csv', 'heatpad_I_94.csv', 'kettle_ricky_39.csv', 'hairdryer-high_ricky_52.csv', 'laptop_will_145.csv', 'laptop_will_160.csv', 'hairdryer-high_ricky_3.csv', 'laptop_will_1.csv', 'hairdryer-high_ricky_68.csv', 'kettle_ricky_60.csv', 'cell_ricky_15.csv', 'heatpad_I_83.csv', 'heatpad_I_24.csv', 'heatpad_I_10.csv', 'hairdryer-high_ricky_14.csv', 'laptop_will_191.csv', 'laptop_will_22.csv', 'laptop_will_108.csv', 'laptop_will_143.csv', 'laptop_will_138.csv', 'laptop_will_151.csv', 'hairdryer-high_ricky_23.csv', 'heatpad_I_43.csv', 'hairdryer-high_ricky_9.csv', 'heatpad_II_13.csv', 'hairdryer-high_ricky_45.csv', 'kettle_ricky_75.csv', 'laptop_will_27.csv', 'laptop_ricky_10.csv', 'kettle_ricky_73.csv', 'inductioncooktop_ricky_11.csv', 'heatpad_I_26.csv', 'hairdryer-high_ricky_84.csv', 'kettle_ricky_59.csv', 'kettle_ricky_37.csv', 'laptop_will_34.csv', 'kettle_ricky_52.csv', 'hairdryer-high_ricky_2.csv', 'cell_ricky_10.csv', 'heatpad_III_15.csv', 'laptop_will_123.csv', 'heatpad_I_6.csv', 'inductioncooktop_ricky_70.csv', 'kettle_ricky_96.csv', 'laptop_ricky_3.csv', 'LEDpicture_ricky_18.csv', 'laptop_will_188.csv', 'heatpad_I_81.csv', 'hairdryer-high_ricky_8.csv', 'laptop_will_68.csv', 'laptop_will_46.csv', 'hairdryer-low_ricky_8.csv', 'kettle_ricky_7.csv', 'laptop_will_157.csv', 'kettle_ricky_67.csv', 'laptop_will_75.csv', 'kettle_ricky_76.csv', 'LEDpicture_ricky_2.csv', 'laptop_will_40.csv', 'laptop_will_177.csv', 'kettle_ricky_1.csv', 'laptop_ricky_15.csv', 'hairdryer-low_ricky_7.csv', 'hairdryer-high_ricky_73.csv', 'Icon\\r', 'laptop_will_71.csv', 'laptop_will_165.csv', 'inductioncooktop_ricky_75.csv', 'laptop_ricky_9.csv', 'hairdryer-high_ricky_18.csv', 'hairdryer-high_ricky_58.csv', 'laptop_will_26.csv', 'cell_ricky_20.csv', 'laptop_will_100.csv', 'laptop_will_172.csv', 'laptop_will_72.csv', 'inductioncooktop_ricky_52.csv', 'inductioncooktop_ricky_95.csv', 'kettle_ricky_9.csv', 'laptop_will_196.csv', 'laptop_will_54.csv', 'hairdryer-high_ricky_38.csv', 'heatpad_I_89.csv', 'hairdryer-low_ricky_1 (1).csv', 'hairdryer-high_ricky_50.csv', 'laptop_will_73.csv', 'cell_ricky_1.csv', 'inductioncooktop_ricky_47.csv', 'kettle_ricky_31.csv', 'cell_ricky_2.csv', 'kettle_ricky_70.csv', 'laptop_will_35.csv', 'heatpad_II_3.csv', 'hairdryer-high_ricky_79.csv', 'hairdryer-high_ricky_97.csv', 'laptop_will_24.csv', 'kettle_ricky_11.csv', 'hairdryer-high_ricky_42.csv', 'laptop_will_66.csv', 'kettle_ricky_85.csv', 'cell_ricky_11.csv', 'kettle_ricky_22.csv', 'laptop_will_155.csv', 'heatpad_I_44.csv', 'hairdryer-high_ricky_1.csv', 'heatpad_I_5.csv', 'heatpad_I_64.csv', 'laptop_will_82.csv', 'cell_ricky_12.csv', 'heatpad_I_69.csv', 'cell_ricky_6.csv', 'heatpad_I_22.csv', 'hairdryer-high_ricky_47.csv', 'kettle_ricky_100.csv', 'laptop_will_137.csv', 'laptop_will_105.csv', 'laptop_will_127.csv', 'laptop_will_128.csv', 'kettle_ricky_21.csv', 'heatpad_I_54.csv', 'heatpad_I_66.csv', 'heatpad_I_39.csv', 'inductioncooktop_ricky_57.csv', 'inductioncooktop_ricky_59.csv', 'laptop_will_166.csv', 'heatpad_I_35.csv', 'heatpad_I_90.csv', 'laptop_will_91.csv', 'hairdryer-high_ricky_71.csv', 'heatpad_I_82.csv', 'inductioncooktop_ricky_78.csv', 'heatpad_II_11.csv', 'hairdryer-high_ricky_53.csv', 'inductioncooktop_ricky_31.csv', 'inductioncooktop_ricky_56.csv', 'kettle_ricky_46.csv', 'kettle_ricky_78.csv', 'heatpad_II_10.csv', 'kettle_ricky_16.csv', 'heatpad_I_3.csv', 'hairdryer-high_ricky_20.csv', 'laptop_will_14.csv', 'inductioncooktop_ricky_41.csv', 'LEDpicture_ricky_17.csv', 'laptop_will_92.csv', 'hairdryer-low_ricky_10.csv', 'hairdryer-low_ricky_19.csv', 'heatpad_I_32.csv', 'hairdryer-high_ricky_35.csv', 'kettle_ricky_91.csv', 'inductioncooktop_ricky_40.csv', 'kettle_ricky_95.csv', 'heatpad_I_70.csv', 'inductioncooktop_ricky_33.csv', 'inductioncooktop_ricky_1.csv', 'laptop_will_88.csv', 'laptop_will_170.csv', 'heatpad_I_21.csv', 'laptop_ricky_1.csv', 'LEDpicture_ricky_6.csv', 'hairdryer-high_ricky_86.csv', 'heatpad_II_9.csv', 'inductioncooktop_ricky_5.csv', 'laptop_will_87.csv', 'heatpad_I_47.csv', 'kettle_ricky_55.csv', 'kettle_ricky_89.csv', 'laptop_will_51.csv', 'inductioncooktop_ricky_93.csv', 'heatpad_II_6.csv', 'laptop_will_200.csv', 'heatpad_I_17.csv', 'inductioncooktop_ricky_6.csv', 'laptop_will_120.csv', 'laptop_will_187.csv', 'inductioncooktop_ricky_51.csv', 'laptop_will_89.csv', 'inductioncooktop_ricky_100.csv', 'kettle_ricky_17.csv', 'inductioncooktop_ricky_94.csv', 'hairdryer-high_ricky_93.csv', 'hairdryer-high_ricky_82.csv', 'heatpad_I_56.csv', 'kettle_ricky_42.csv', 'kettle_ricky_79.csv', 'laptop_will_193.csv', 'LEDpicture_ricky_20.csv', 'laptop_will_113.csv', 'laptop_will_6.csv', 'kettle_ricky_18.csv', 'heatpad_I_53.csv', 'heatpad_I_96.csv', 'hairdryer-high_ricky_13.csv', 'heatpad_I_20.csv', 'inductioncooktop_ricky_43.csv', 'laptop_will_102.csv', 'inductioncooktop_ricky_49.csv', 'kettle_ricky_4.csv', 'inductioncooktop_ricky_15.csv', 'hairdryer-high_ricky_21.csv', 'hairdryer-low_ricky_11.csv', 'laptop_will_76.csv', 'laptop_will_189.csv', 'laptop_will_69.csv', 'heatpad_I_77.csv', 'kettle_ricky_66.csv', 'laptop_ricky_5.csv', 'laptop_will_182.csv', 'laptop_will_147.csv', 'hairdryer-high_ricky_40.csv', 'kettle_ricky_99.csv', 'laptop_will_31.csv', 'heatpad_I_23.csv', 'kettle_ricky_24.csv', 'kettle_ricky_40.csv', 'laptop_will_152.csv', 'hairdryer-high_ricky_17.csv', 'heatpad_I_86.csv', 'laptop_will_161.csv', 'inductioncooktop_ricky_29.csv', 'inductioncooktop_ricky_65.csv', 'kettle_ricky_33.csv', 'kettle_ricky_38.csv', 'heatpad_I_63.csv', 'laptop_will_171.csv', 'hairdryer-high_ricky_91.csv', 'laptop_will_50.csv', 'hairdryer_ricky_21.csv', 'laptop_will_29.csv', 'hairdryer-high_ricky_61.csv', 'hairdryer-high_ricky_33.csv', 'inductioncooktop_ricky_86.csv', 'heatpad_III_18.csv', 'laptop_will_101.csv', 'heatpad_I_34.csv', 'hairdryer-high_ricky_12.csv', 'kettle_ricky_13.csv', 'hairdryer-high_ricky_46.csv', 'heatpad_I_52.csv', 'hairdryer-high_ricky_65.csv', 'laptop_ricky_6.csv', 'kettle_ricky_2.csv', 'laptop_will_62.csv', 'hairdryer-high_ricky_76.csv', 'laptop_will_186.csv', 'heatpad_I_80.csv', 'inductioncooktop_ricky_20.csv', 'kettle_ricky_62.csv', 'heatpad_I_71.csv', 'kettle_ricky_65.csv', 'inductioncooktop_ricky_77.csv', 'heatpad_II_12.csv', 'laptop_will_64.csv', 'LEDpicture_ricky_19.csv', 'inductioncooktop_ricky_22.csv', 'laptop_will_192.csv', 'hairdryer-high_ricky_92.csv', 'hairdryer-high_ricky_31.csv', 'laptop_will_78.csv', 'inductioncooktop_ricky_4.csv', 'heatpad_I_87.csv', 'kettle_ricky_26.csv', 'heatpad_I_50.csv', 'laptop_ricky_17.csv', 'heatpad_I_79.csv', 'heatpad_III_3.csv', 'hairdryer-high_ricky_48.csv', 'kettle_ricky_51.csv', 'hairdryer-high_ricky_28.csv', 'laptop_will_60.csv', 'heatpad_I_42.csv', 'laptop_will_4.csv', 'cell_ricky_14.csv', 'heatpad_I_88.csv', 'kettle_ricky_87.csv', 'hairdryer-high_ricky_55.csv', 'heatpad_I_61.csv', 'laptop_will_109.csv', 'kettle_ricky_90.csv', 'hairdryer-high_ricky_11.csv', 'kettle_ricky_47.csv', 'hairdryer-high_ricky_22.csv', 'heatpad_I_11.csv', 'laptop_will_149.csv', 'kettle_geoff_1.csv', 'laptop_ricky_7.csv', 'kettle_ricky_41.csv', 'heatpad_I_33.csv', 'kettle_ricky_82.csv', 'kettle_ricky_68.csv', 'hairdryer-low_ricky_9.csv', 'laptop_ricky_14.csv', 'inductioncooktop_ricky_79.csv', 'laptop_will_178.csv', 'kettle_ricky_92.csv', 'laptop_ricky_16.csv', 'laptop_will_176.csv', 'inductioncooktop_ricky_34.csv', 'cell_ricky_13.csv', 'inductioncooktop_ricky_73.csv', 'LEDpicture_ricky_5.csv', 'inductioncooktop_ricky_44.csv', 'laptop_will_162.csv', 'inductioncooktop_ricky_82.csv', 'heatpad_I_75.csv', 'hairdryer-high_ricky_39.csv', 'laptop_will_8.csv', 'hairdryer-high_ricky_95.csv', 'hairdryer-high_ricky_43.csv', 'heatpad_I_7.csv', 'heatpad_I_14.csv', 'laptop_will_43.csv', 'cell_ricky_4.csv', 'laptop_will_190.csv', 'laptop_will_118.csv', 'inductioncooktop_ricky_90.csv', 'heatpad_I_59.csv', 'heatpad_I_76.csv', 'heatpad_I_37.csv', 'laptop_will_135.csv', 'laptop_will_159.csv', 'inductioncooktop_ricky_19.csv', 'hairdryer-low_ricky_4.csv', 'heatpad_III_17.csv', 'LEDpicture_ricky_16.csv', 'inductioncooktop_ricky_2.csv', 'inductioncooktop_ricky_25.csv', 'inductioncooktop_ricky_10.csv', 'kettle_ricky_77.csv', 'inductioncooktop_ricky_27.csv', 'inductioncooktop_ricky_88.csv', 'hairdryer-high_ricky_34.csv', 'heatpad_I_68.csv', 'hairdryer-high_ricky_70.csv', 'heatpad_III_4.csv', 'heatpad_I_12.csv', 'laptop_will_9.csv', 'kettle_ricky_53.csv', 'laptop_will_15.csv', 'hairdryer-high_ricky_77.csv', 'LEDpicture_ricky_15.csv', 'laptop_will_63.csv', 'heatpad_II_19.csv', 'hairdryer-high_ricky_66.csv', 'inductioncooktop_ricky_91.csv', 'hairdryer-high_ricky_64.csv', 'hairdryer-low_ricky_2.csv', 'laptop_will_67.csv', 'laptop_will_136.csv', 'kettle_ricky_64.csv', 'laptop_will_30.csv', 'kettle_ricky_25.csv', 'hairdryer-high_ricky_4.csv', 'laptop_will_146.csv', 'laptop_will_110.csv', 'laptop_will_140.csv', 'inductioncooktop_ricky_38.csv', 'inductioncooktop_ricky_98.csv', 'inductioncooktop_ricky_67.csv', 'heatpad_I_65.csv', 'hairdryer-high_ricky_100.csv', 'hairdryer-high_ricky_63.csv', 'heatpad_II_15.csv', 'hairdryer-low_ricky_20.csv', 'heatpad_I_40.csv', 'inductioncooktop_ricky_35.csv', 'laptop_will_37.csv', 'heatpad_I_97.csv', 'heatpad_I_27.csv', 'kettle_ricky_44.csv', 'laptop_will_93.csv', 'heatpad_I_46.csv', 'inductioncooktop_ricky_45.csv', 'kettle_ricky_48.csv', 'kettle_ricky_80.csv', 'heatpad_II_2.csv', 'laptop_will_25.csv', 'heatpad_I_72.csv', 'hairdryer-high_ricky_74.csv', 'hairdryer-low_ricky_3.csv', 'heatpad_I_67.csv', 'laptop_will_79.csv', 'inductioncooktop_ricky_80.csv', 'heatpad_III_5.csv', 'laptop_will_106.csv', 'heatpad_III_7.csv', 'heatpad_I_45.csv', 'heatpad_II_1.csv', 'laptop_will_144.csv', 'heatpad_I_78.csv', 'heatpad_I_98.csv', 'inductioncooktop_ricky_58.csv', 'heatpad_III_12.csv', 'cell_ricky_16.csv', 'laptop_will_175.csv', 'kettle_ricky_45.csv', 'kettle_ricky_19.csv', 'hairdryer-high_ricky_5.csv', 'laptop_will_126.csv', 'kettle_ricky_30.csv', 'laptop_will_36.csv', 'heatpad_III_1.csv', 'laptop_will_85.csv', 'inductioncooktop_ricky_84.csv', 'hairdryer-high_ricky_94.csv', 'inductioncooktop_ricky_13.csv', 'heatpad_I_2.csv', 'hairdryer-low_ricky_14.csv', 'heatpad_III_8.csv', 'kettle_ricky_81.csv', 'kettle_ricky_29.csv', 'kettle_ricky_27.csv', 'kettle_ricky_32.csv', 'heatpad_I_93.csv', 'kettle_ricky_8.csv', 'LEDpicture_ricky_14.csv', 'kettle_ricky_50.csv', 'laptop_will_99.csv', 'hairdryer-low_ricky_18.csv', 'kettle_ricky_69.csv', 'heatpad_III_11.csv', 'inductioncooktop_ricky_54.csv', 'laptop_will_57.csv', 'kettle_ricky_94.csv', 'laptop_will_167.csv', 'inductioncooktop_ricky_21.csv', 'hairdryer-high_ricky_15.csv', 'laptop_will_58.csv', 'kettle_ricky_15.csv', 'kettle_ricky_14.csv', 'laptop_will_59.csv', 'laptop_will_44.csv', 'laptop_will_148.csv', 'laptop_ricky_4.csv', 'heatpad_I_41.csv', 'hairdryer-high_ricky_87.csv', 'laptop_will_122.csv', 'laptop_will_13.csv', 'hairdryer-high_ricky_75.csv', 'heatpad_I_73.csv', 'laptop_will_49.csv', 'kettle_ricky_74.csv', 'laptop_will_181.csv', 'laptop_will_2.csv', 'hairdryer-low_ricky_17.csv', 'laptop_will_23.csv', 'heatpad_II_5.csv', 'inductioncooktop_ricky_61.csv', 'LEDpicture_ricky_8.csv', 'inductioncooktop_ricky_24.csv', 'hairdryer-high_ricky_59.csv', 'cell_ricky_5.csv', 'heatpad_I_19.csv', 'laptop_will_84.csv', 'hairdryer-high_ricky_51.csv', 'laptop_will_47.csv', 'heatpad_II_14.csv', 'laptop_will_39.csv', 'heatpad_I_60.csv', 'hairdryer-low_ricky_5.csv', 'inductioncooktop_ricky_85.csv', 'kettle_ricky_83.csv', 'inductioncooktop_ricky_26.csv', 'heatpad_I_8.csv', 'inductioncooktop_ricky_87.csv', 'hairdryer-high_ricky_96.csv', 'heatpad_II_20.csv', 'heatpad_I_62.csv', 'laptop_will_11.csv', 'heatpad_I_48.csv', 'inductioncooktop_ricky_9.csv', 'hairdryer-high_ricky_83.csv', 'hairdryer-high_ricky_98.csv', 'kettle_ricky_34.csv', 'laptop_will_10.csv', 'heatpad_II_17.csv', 'laptop_will_70.csv', 'hairdryer-high_ricky_27.csv', 'heatpad_I_100.csv', 'inductioncooktop_ricky_89.csv', 'laptop_will_55.csv', 'kettle_ricky_6.csv', 'inductioncooktop_ricky_7.csv', 'kettle_ricky_10.csv', 'laptop_will_158.csv', 'laptop_will_81.csv', 'hairdryer-low_ricky_12.csv', 'hairdryer-high_ricky_44.csv', 'hairdryer-high_ricky_26.csv', 'LEDpicture_ricky_10.csv', 'inductioncooktop_ricky_97.csv', 'laptop_will_116.csv', 'laptop_will_156.csv', 'laptop_will_184.csv', 'laptop_will_131.csv', 'heatpad_I_92.csv', 'inductioncooktop_ricky_8.csv', 'heatpad_I_30.csv', 'laptop_will_150.csv', 'heatpad_III_14.csv', 'inductioncooktop_ricky_28.csv', 'hairdryer-high_ricky_90.csv', 'hairdryer-high_ricky_72.csv', 'kettle_ricky_12.csv', 'inductioncooktop_ricky_71.csv', 'inductioncooktop_ricky_48.csv']\n"],"name":"stdout"}]},{"metadata":{"id":"Y3Xnun_CFL9G","colab_type":"code","colab":{}},"cell_type":"code","source":["model_dir_path = '/content/gdrive/My Drive/Models'\n","# Check that this directory exists.\n","os.path.isdir(model_dir_path)\n","\n","# Make a new directory for our MNIST models\n","example_path = os.path.join(model_dir_path, 'emonpi')\n","print('example_path',example_path)\n","\n","if not os.path.isdir(example_path):\n"," os.makedirs(example_path)\n"," print('example path created')\n","\n","checkpoint_path = os.path.join(example_path, 'lq_model_1.h5')\n","print('checkpoint_path',checkpoint_path)\n","# Create a keras callback that saves our model during training.\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MudzCN5ubEVV","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def convert_str_to_int(str_list,keys): #converts labels from string to integer \n","  list_int = []\n","  for i in str_list:\n","    list_int.append(appliance_dict[i])\n","  return list_int\n","\n","def convert_int_to_str(integer,keys):\n","  for appliance,index in keys.items():\n","    if index == integer:\n","        #print(integer,appliance)\n","        return appliance\n","      \n","def make3D(array,n): #adds channels to array to simulate an image\n","  return (np.stack((array,)*n, axis=-1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JsE0rl3_31dc","colab_type":"text"},"cell_type":"markdown","source":["------------------------------------------------------------------\n","# NOISE AND DATA WARPING FUNCTIONS\n","------------------------------------------------------------------"]},{"metadata":{"id":"QtKKsqYG30ZK","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from random import randint\n","import tensorflow as tf\n","\n","\n","def warp_trim_pad(feature, length):\n","    \"\"\"\n","    Helper function that trims or pads arrays if they have length > length\n","    :param feature- a feature to be trimmed or padded with zeros, length - a number specifying the length to be trimmed to\n","    :return: the same feature array, trimmed or padded with zeros\n","    \"\"\"\n","    padded_feature = []\n","    if (len(feature) > length):\n","        padded_feature = feature[0:length]\n","    elif (len(feature) < length):\n","        padding = np.zeros(length-len(feature))\n","        padded_feature = np.concatenate([feature, padding])\n","    return np.array(padded_feature).astype('float32')\n","\n","def warp_shrink(feat, percentage):\n","    \"\"\"\n","    Takes in a feature in the form of a numpy array, takes a random sample from the array and shrinks it by half.\n","    The size of the random sample is equal to 1/10 of the size of the source array.  Replaces sample with pair-wise average of sample indices.\n","    Approach inspired from this paper: https://aaltd16.irisa.fr/files/2016/08/AALTD16_paper_9.pdf\n","    :param feat - feature to be warped in form of numpy array, percentage - proportion of sample to be warped\n","    :return: Feature array with sample shrank\n","    \"\"\"\n","    #Get length of array\n","    sample_range = feat.shape[0]\n","\n","    #Calculate length of sample\n","    sample_tenth = int(sample_range*percentage)\n","\n","    #Shift sample length to be even to allow for pair-wise averages\n","    if sample_tenth % 2 == 1:\n","        sample_tenth += 1\n","\n","    # Generate random index for start of sample\n","    sample_index = randint(0, sample_range - (sample_tenth))\n","\n","    #Shift sample index to be even to allow for pair-wise averages\n","    if sample_index % 2 == 1:\n","        sample_index += 1\n","\n","    # If sample will overlap with end of sample, shift sample back to fit in array\n","    if (sample_index+sample_tenth) > len(feat):\n","        sample_index = len(feat)-sample_tenth\n","        sample = feat[sample_index:int(sample_index + sample_tenth)]\n","    else:\n","        # Subset feature array into sample\n","        sample = feat[sample_index:int(sample_index + sample_tenth)]\n","\n","    #Subset feature array into sample\n","    sample = feat[sample_index:int(sample_index+sample_tenth)]\n","    # print('sample index', sample_index)\n","    # print('sample_tenth', sample_tenth)\n","    # print('sample length', len(sample))\n","\n","    avg_arr = []\n","    for i in range(0, sample_tenth, 2): #iterate thru pairs of elements in sample, calculating average\n","        avg = (sample[i]+sample[i+1])/2\n","        avg_arr.append(avg)\n","\n","    avg_arr = np.array(avg_arr)\n","\n","    # Replace sample with shrunk average of samples\n","    feat_before = feat[:sample_index]\n","    feat_after = feat[int(sample_index+sample_tenth):]\n","    feat_transformed = np.concatenate([feat_before,avg_arr,feat_after])\n","\n","    return warp_trim_pad(feat_transformed,50)\n","\n","def warp_stretch(feat, percentage):\n","    \"\"\"\n","    Takes in a feature in the form of a numpy array, takes a random sample from the array and shrinks it by half.\n","    The size of the random sample is equal to 1/10 of the size of the source array.  Appends pair-wise average of every other sample index.\n","    Approach inspired from this paper: https://aaltd16.irisa.fr/files/2016/08/AALTD16_paper_9.pdf\n","    :param: feat - feature to be warped in form of numpy array, percentage - proportion of sample to be warped\n","    :return: Feature array with sample stretched\n","    \"\"\"\n","\n","    # Get length of array\n","    sample_range = feat.shape[0]\n","\n","    # Calculate length of sample\n","    sample_tenth = int(sample_range * percentage)\n","\n","    # Shift sample length to be even to allow for pair-wise averages\n","    if sample_tenth % 2 == 1:\n","        sample_tenth += 1\n","\n","    # Generate random index for start of sample\n","    sample_index = randint(0, sample_range - (sample_tenth))\n","\n","    # Shift sample index to be even to allow for pair-wise averages\n","    if sample_index % 2 == 1:\n","        sample_index += 1\n","\n","    # If sample will overlap with end of sample, shift sample back to fit in array\n","    if (sample_index+sample_tenth) > len(feat):\n","        sample_index = len(feat)-sample_tenth\n","        sample = feat[sample_index:int(sample_index + sample_tenth)]\n","    else:\n","        # Subset feature array into sample\n","        sample = feat[sample_index:int(sample_index + sample_tenth)]\n","    # print('sample index', sample_index)\n","    # print('sample_tenth', sample_tenth)\n","    # print('sample length', len(sample))\n","\n","    stretch_arr = []\n","    for i in range(0, sample_tenth,2): #Iterate thru each pair of elements in the sample\n","        if (i > 0 & i <= sample_tenth-1):  #Get average of previous index and current index\n","            avg_prev = (sample[i-1]+sample[i])/2\n","            stretch_arr.append(avg_prev)\n","        # print(len(sample), 'current index', i)\n","        avg = (sample[i]+sample[i+1])/2 #Get average of current index\n","        stretch_arr.append(sample[i])\n","        stretch_arr.append(avg)\n","        stretch_arr.append(sample[i+1])\n","\n","    stretch_arr = np.array(stretch_arr)\n","\n","    #Replace sample with stretched sample\n","    feat_before = feat[:sample_index]\n","    feat_after = feat[int(sample_index+sample_tenth):]\n","    feat_transformed = np.concatenate([feat_before,stretch_arr,feat_after])\n","    return warp_trim_pad(feat_transformed,50)\n","\n","def warp_features(observation, percentage):\n","    \"\"\"\"\n","    Applies warp shrink/stretch at random to numpy array of feature arrays.\n","    Parameters: observation - numpy array of arrays, percentage - proportion of sample to be warped\n","    Returns: warped numpy array of arrays\n","    \"\"\"\n","    new_obs = []\n","    for feature in range(0,observation.shape[1]): #Iterates thru each feature array\n","        if (randint(0,1) > 0):\n","            if (len(observation[:,feature])>50):\n","              print()\n","            warped_feature = warp_stretch(observation[:,feature],percentage) # applies stretch randomly\n","        else:\n","            warped_feature = warp_shrink(observation[:,feature],percentage) # else applies shrink\n","        new_obs.append(warped_feature)\n","\n","    return np.asarray(new_obs)\n","\n","def pad_features(observation, length):\n","    \"\"\"\"\n","    Applies trim/pad to numpy array of feature arrays.\n","    Parameters: observation - numpy array of arrays, length - length to trim arrays to\n","    Returns: padded numpy array of arrays\n","    \"\"\"\n","    new_obs = []\n","    for feature in range(0,observation.shape[1]):\n","        warped_feature = warp_trim_pad(observation[:,feature],length) # applies trim/pad to feature array\n","        new_obs.append(warped_feature)\n","\n","    return np.asarray(new_obs)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TdwvXI6E0zpO","colab_type":"text"},"cell_type":"markdown","source":["------------------------------------------------------------------\n","# LOAD AND PREPROCESS DATA\n","------------------------------------------------------------------"]},{"metadata":{"id":"JDeSfrjpG049","colab_type":"code","outputId":"0cd0c705-2a60-4547-ddf6-3b9bb7abec22","executionInfo":{"status":"ok","timestamp":1552581282725,"user_tz":420,"elapsed":8506,"user":{"displayName":"Louis Quicksell","photoUrl":"","userId":"02987867429841815458"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["data_path = training_path #\"/Users/willbuchanan/Google Drive/GIX/ricky-will-louis/data/training/\"\n","\n","\n","def process_data(filepath,labels=[],data=[]):\n","#     min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))  # define range for scale operation\n","    vars = ['time', 'power_factor', 'phase_angle', 'power_real', 'power_reactive', 'power_apparent', 'vrms', 'irms']\n","    cwd = os.chdir(filepath)\n","    for appliance_type in os.listdir(cwd):\n","        #print(\"\\n\",appliance_type)\n","        if appliance_type.endswith('.csv'):\n","            #split_file = str(filename).split(\"_\")\n","            #print(appliance_type)\n","            app_df = pd.read_csv(filepath + appliance_type)\n","            app_df.columns = app_df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n","            app_arr = []\n","            for i in range(0, len(app_df['time'])):\n","                data_row = []\n","                for column in vars:\n","                    data_row.append(app_df[column][i])\n","                app_arr.append(data_row)\n","            # app_arr_normalized = min_max_scaler.fit_transform(app_arr) #must scale when in 2D array form\n","            data.append(np.array(app_arr))\n","            label = str(app_df['app_name'][0]).split('-')[0]\n","            if label == 'ricky':\n","              print(appliance_type)\n","#             print('label:',label)\n","            labels.append(label)#str(app_df['app_name'][0]).split('-')[0])\n","    return labels,data\n","\n","dataset_labels_str,dataset_data = process_data(data_path) #extracts CSV into labels & data\n","dataset_data_np = tf.keras.preprocessing.sequence.pad_sequences(np.asarray(dataset_data), value=0, maxlen=50) #pads length to 50\n","print(set(dataset_labels_str))\n","\n","#--------------------extracts labels & assigns to appliance dictionary--------------------\n","\n","\n","labelset = list(set(dataset_labels_str))\n","\n","appliance_dict = {}\n","for i,appliance in enumerate(labelset):\n","  appliance_dict[appliance] = i\n","print('appliance_dict',appliance_dict)\n","\n","dataset_labels_int = convert_str_to_int(dataset_labels_str, appliance_dict) #converts from strings to integer\n","print('label sample:',dataset_labels_int[:5],dataset_labels_str[:5])\n","\n","min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))  # define range for scale operation\n","scaled_dataset = []\n","for i,sample in enumerate(dataset_data_np):\n","    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))  # define range for scale operation\n","    app_arr_normalized = min_max_scaler.fit_transform(sample) #must scale when in 2D array form\n","    scaled_dataset.append(app_arr_normalized)\n","\n","scaled_dataset = np.array(scaled_dataset)\n","scaled_dataset_3D = make3D(scaled_dataset,3)\n","\n","# -- Split data into training and test subsets\n","data_train, data_test, labels_train, labels_test = train_test_split(scaled_dataset, dataset_labels_int, test_size=0.20)#, random_state=42) #initialize random_state to get same result every time\n","print('train data shape:',data_train.shape,'# of train labels:',len(labels_train),'\\nunique train labels:',set(labels_train))\n","print('test data shape:',data_test.shape,'# of test labels:',len(labels_test),'\\nunique test labels:',set(labels_test))\n","\n","\n","num_labels = len(set(labels_train))\n","print(num_labels,'labels created')\n","\n","# Let's first make sure the shape and type of our data is correct.\n","# Convert data to float32 datatype and labels to int64 datatype.\n","train_data = tf.cast(data_train, tf.float32)\n","train_labels = tf.cast(labels_train, tf.int64)\n","test_data = tf.cast(data_test, tf.float32)\n","test_labels = tf.cast(labels_test, tf.int64)\n","\n","# When working with images, TensorFlow needs them to be shape [H, W, C], but\n","# # our data is just [H, W] right now since its black and white. Let's add a extra channel axis.\n","train_data = train_data[..., tf.newaxis]\n","test_data = test_data[..., tf.newaxis]\n","\n","# # Now were ready to create Tensorflow Datasets!\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n","\n","\n","# Finally, let's shuffle our training data and batch it so its more efficient.\n","train_dataset = train_dataset.shuffle(20).batch(1000)\n","test_dataset = test_dataset.shuffle(20).batch(1000)\n","\n","\n","# -- Transform data labels from string to int for use in model\n","le = preprocessing.LabelEncoder()\n","le.fit(list(set(labels_train)))\n","train_labels = le.transform(labels_train)\n","test_labels = le.transform(labels_test)\n","\n","\n","# # -- Reshape train data for use in model\n","nsamples, nx, ny = data_train.shape\n","train_data_2d = data_train.reshape((nsamples,nx*ny))\n","\n","# # -- Reshape test data for use in model\n","nsamples, nx, ny  = data_test.shape\n","test_data_2d = data_test.reshape((nsamples,nx*ny))\n","\n","nsamples, nx, ny  = scaled_dataset.shape\n","dataset_data_np_2d = scaled_dataset.reshape((nsamples,nx*ny))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'cell', 'kettle', 'inductioncooktop', 'laptop', 'hairdryer', 'LEDpicture', 'heatpad'}\n","appliance_dict {'cell': 0, 'kettle': 1, 'inductioncooktop': 2, 'laptop': 3, 'hairdryer': 4, 'LEDpicture': 5, 'heatpad': 6}\n","label sample: [6, 4, 2, 1, 3] ['heatpad', 'hairdryer', 'inductioncooktop', 'kettle', 'laptop']\n","train data shape: (576, 50, 8) # of train labels: 576 \n","unique train labels: {0, 1, 2, 3, 4, 5, 6}\n","test data shape: (144, 50, 8) # of test labels: 144 \n","unique test labels: {0, 1, 2, 3, 4, 5, 6}\n","7 labels created\n"],"name":"stdout"}]},{"metadata":{"id":"NrSW8LIBZJ9q","colab_type":"code","outputId":"305be8e5-6a7e-4598-beb5-fad3e2d970b2","executionInfo":{"status":"ok","timestamp":1552581285872,"user_tz":420,"elapsed":910,"user":{"displayName":"Louis Quicksell","photoUrl":"","userId":"02987867429841815458"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"cell_type":"code","source":["print(data_train.shape)\n","print(list(set(labels_train)))\n","print(train_labels)\n","print(scaled_dataset.shape)\n","print(dataset_data_np_2d.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(576, 50, 8)\n","[0, 1, 2, 3, 4, 5, 6]\n","[3 5 1 3 2 6 1 5 4 1 3 6 3 6 1 3 6 4 0 6 2 6 0 6 6 3 3 6 6 2 3 4 3 4 4 4 5\n"," 4 3 1 1 6 2 6 4 3 3 6 0 6 6 1 4 1 3 4 5 6 6 2 3 2 4 6 3 2 6 4 6 6 0 4 6 6\n"," 6 4 6 4 4 6 6 3 2 1 0 1 5 3 0 3 1 2 2 3 6 4 2 1 3 4 4 3 2 3 4 3 6 3 4 3 6\n"," 3 1 2 3 1 6 6 3 3 1 0 3 1 1 5 4 2 0 4 6 4 3 3 2 6 4 3 3 3 2 4 3 1 3 0 6 3\n"," 4 3 3 6 1 6 3 4 1 6 1 2 2 3 3 1 2 4 3 4 4 6 3 4 2 3 4 3 3 4 3 1 3 6 3 4 2\n"," 3 3 3 6 2 6 3 4 4 2 3 3 3 6 6 4 1 6 1 2 6 3 2 3 3 1 2 6 3 3 6 4 1 2 1 2 3\n"," 3 1 3 6 4 2 4 6 4 2 1 6 4 3 6 1 3 4 6 1 2 2 2 3 1 4 6 2 3 1 6 1 2 4 3 5 3\n"," 0 3 2 4 5 3 2 1 1 2 4 3 6 3 1 4 4 3 4 1 3 3 1 0 1 4 1 6 6 3 1 6 5 4 6 1 2\n"," 2 6 1 1 6 2 3 1 3 4 2 2 6 1 4 1 1 2 6 3 2 6 1 1 4 4 5 2 1 1 3 1 6 0 3 1 6\n"," 3 3 4 6 2 4 2 4 1 3 3 3 3 2 3 2 2 6 2 6 3 3 4 2 3 3 4 1 3 3 3 3 4 3 4 5 1\n"," 1 3 6 3 5 2 3 3 4 2 3 3 6 2 1 2 3 3 6 2 6 3 4 3 3 3 6 3 2 4 3 1 1 2 3 1 2\n"," 3 3 1 3 3 4 1 4 4 6 3 4 2 1 1 6 4 4 1 2 3 3 2 2 3 3 5 6 2 6 6 3 6 6 4 4 6\n"," 4 1 3 6 3 2 3 3 4 3 1 6 3 4 6 4 3 6 6 2 4 6 1 3 2 3 6 6 5 3 3 2 4 4 3 3 3\n"," 0 1 3 0 4 4 3 3 6 1 2 2 3 3 3 3 1 4 0 4 1 4 2 0 6 2 4 6 4 6 1 6 4 3 3 1 4\n"," 2 1 4 3 2 3 3 2 2 3 5 3 6 4 2 6 1 6 3 3 3 1 1 4 4 6 3 4 4 3 1 6 6 0 6 3 3\n"," 4 6 3 4 2 1 6 6 6 1 4 6 6 6 3 2 6 3 3 3 4]\n","(720, 50, 8)\n","(720, 400)\n"],"name":"stdout"}]},{"metadata":{"id":"ucQQuqxr03DL","colab_type":"text"},"cell_type":"markdown","source":["------------------------------------------------------------------\n","# SUPERVISED LEARNING MODELS \n","------------------------------------------------------------------"]},{"metadata":{"id":"hzSDl77yNjZP","colab_type":"code","colab":{}},"cell_type":"code","source":["inv_map"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KFAPsQCvzFIv","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC, SVC\n","from sklearn.model_selection import cross_val_score, GridSearchCV\n","from sklearn import svm, ensemble\n","\n","def svc_param_selection(X, y, nfolds):\n","    Cs = [0.001, 0.01, 0.1, 1, 10, 100]\n","    gammas = [0.001, 0.01, 0.1, 1]\n","    param_grid = {'C': Cs, 'gamma' : gammas}\n","    grid_search = GridSearchCV(svm.SVC(kernel='rbf',decision_function_shape='ovo'), param_grid, cv=nfolds)\n","    grid_search.fit(X, y)\n","    grid_search.best_params_\n","    return grid_search.best_params_\n","\n","\n","def gbm_param_selection(x,y,nfolds):\n","    parameters = {\n","      \"loss\":[\"deviance\"],\n","      \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n","      \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n","      \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n","      \"max_depth\":[3,5,8],\n","      \"max_features\":[\"log2\",\"sqrt\"],\n","      \"criterion\": [\"friedman_mse\",  \"mae\"],\n","      \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n","      \"n_estimators\":[10,20]\n","    }\n","    clf = GridSearchCV(ensemble.GradientBoostingClassifier(verbose=1), parameters, cv=nfolds, n_jobs=-1)\n","    clf.fit(x, y)\n","    return clf.best_params_\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"11AnONi_OBqU","colab_type":"code","colab":{}},"cell_type":"code","source":["np.linspace(0.1, 0.5, 5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YuMgUaOA2fNg","colab_type":"code","colab":{}},"cell_type":"code","source":["# print(svc_param_selection(dataset_data_np_2d, dataset_labels_int, 10))\n","print(gbm_param_selection(dataset_data_np_2d, dataset_labels_int, 10))\n","\n","#gbm_params = gbm_param_selection(dataset_data_np_2d, dataset_labels_int, 10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uj2hIQfsa77T","colab_type":"code","colab":{}},"cell_type":"code","source":["gbm_parameters = {\n","        \"loss\":\"deviance\",\n","        \"learning_rate\": 0.2,\n","        \"min_samples_split\":0.2,\n","        \"min_samples_leaf\":0.1,\n","        \"max_depth\":5,\n","        \"max_features\":\"sqrt\",\n","        \"criterion\": \"friedman_mse\",\n","        \"subsample\":1.0,\n","        \"n_estimators\":10\n","        }"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nMNhSL5h2dBr","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.naive_bayes import GaussianNB,MultinomialNB\n","models = [\n","    RandomForestClassifier(n_estimators=250, max_depth=4, random_state=0),\n","    LinearSVC(),\n","    SVC(C = 10, gamma=0.01, decision_function_shape='ovo'),\n","    ensemble.GradientBoostingClassifier(**gbm_parameters, verbose=1),\n","    MultinomialNB(),\n","    GaussianNB(),\n","    LogisticRegression(random_state=0),\n","]\n","CV = 5\n","cv_df = pd.DataFrame(index=range(CV * len(models)))\n","entries = []\n","for model in models:\n","  model_name = model.__class__.__name__\n","  accuracies = cross_val_score(model, dataset_data_np_2d, dataset_labels_int, scoring='accuracy', cv=CV)\n","  for fold_idx, accuracy in enumerate(accuracies):\n","    entries.append((model_name, fold_idx, accuracy))\n","cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NSCl5sEOziET","colab_type":"code","colab":{}},"cell_type":"code","source":["import seaborn as sns\n","\n","sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n","sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n","              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n","plt.show()\n","\n","cv_df.groupby('model_name').accuracy.mean()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r4AiE1p12UDo","colab_type":"code","outputId":"4725d4a6-906a-4d0f-ec05-f439698d332d","executionInfo":{"status":"ok","timestamp":1552583525858,"user_tz":420,"elapsed":905,"user":{"displayName":"Louis Quicksell","photoUrl":"","userId":"02987867429841815458"}},"colab":{"base_uri":"https://localhost:8080/","height":1134}},"cell_type":"code","source":["cv_df.sort_values(by='accuracy', ascending=False)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model_name</th>\n","      <th>fold_idx</th>\n","      <th>accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>17</th>\n","      <td>GradientBoostingClassifier</td>\n","      <td>2</td>\n","      <td>0.951389</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>RandomForestClassifier</td>\n","      <td>2</td>\n","      <td>0.951389</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>RandomForestClassifier</td>\n","      <td>3</td>\n","      <td>0.951389</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>GradientBoostingClassifier</td>\n","      <td>3</td>\n","      <td>0.951389</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>SVC</td>\n","      <td>2</td>\n","      <td>0.944444</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>RandomForestClassifier</td>\n","      <td>4</td>\n","      <td>0.943662</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>SVC</td>\n","      <td>0</td>\n","      <td>0.937931</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>LogisticRegression</td>\n","      <td>2</td>\n","      <td>0.937500</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>LogisticRegression</td>\n","      <td>0</td>\n","      <td>0.931034</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>SVC</td>\n","      <td>1</td>\n","      <td>0.931034</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>RandomForestClassifier</td>\n","      <td>0</td>\n","      <td>0.931034</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LinearSVC</td>\n","      <td>2</td>\n","      <td>0.930556</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>LogisticRegression</td>\n","      <td>1</td>\n","      <td>0.924138</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LinearSVC</td>\n","      <td>1</td>\n","      <td>0.924138</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>LinearSVC</td>\n","      <td>4</td>\n","      <td>0.922535</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>RandomForestClassifier</td>\n","      <td>1</td>\n","      <td>0.917241</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>LinearSVC</td>\n","      <td>0</td>\n","      <td>0.917241</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>LogisticRegression</td>\n","      <td>4</td>\n","      <td>0.915493</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>GradientBoostingClassifier</td>\n","      <td>1</td>\n","      <td>0.910345</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>GradientBoostingClassifier</td>\n","      <td>0</td>\n","      <td>0.910345</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>SVC</td>\n","      <td>3</td>\n","      <td>0.909722</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>LinearSVC</td>\n","      <td>3</td>\n","      <td>0.909722</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>GradientBoostingClassifier</td>\n","      <td>4</td>\n","      <td>0.908451</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>SVC</td>\n","      <td>4</td>\n","      <td>0.908451</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>LogisticRegression</td>\n","      <td>3</td>\n","      <td>0.895833</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>GaussianNB</td>\n","      <td>1</td>\n","      <td>0.882759</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>GaussianNB</td>\n","      <td>0</td>\n","      <td>0.882759</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>MultinomialNB</td>\n","      <td>0</td>\n","      <td>0.868966</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>GaussianNB</td>\n","      <td>2</td>\n","      <td>0.868056</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>GaussianNB</td>\n","      <td>4</td>\n","      <td>0.852113</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>GaussianNB</td>\n","      <td>3</td>\n","      <td>0.847222</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>MultinomialNB</td>\n","      <td>3</td>\n","      <td>0.847222</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>MultinomialNB</td>\n","      <td>2</td>\n","      <td>0.840278</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>MultinomialNB</td>\n","      <td>4</td>\n","      <td>0.802817</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>MultinomialNB</td>\n","      <td>1</td>\n","      <td>0.779310</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    model_name  fold_idx  accuracy\n","17  GradientBoostingClassifier         2  0.951389\n","2       RandomForestClassifier         2  0.951389\n","3       RandomForestClassifier         3  0.951389\n","18  GradientBoostingClassifier         3  0.951389\n","12                         SVC         2  0.944444\n","4       RandomForestClassifier         4  0.943662\n","10                         SVC         0  0.937931\n","32          LogisticRegression         2  0.937500\n","30          LogisticRegression         0  0.931034\n","11                         SVC         1  0.931034\n","0       RandomForestClassifier         0  0.931034\n","7                    LinearSVC         2  0.930556\n","31          LogisticRegression         1  0.924138\n","6                    LinearSVC         1  0.924138\n","9                    LinearSVC         4  0.922535\n","1       RandomForestClassifier         1  0.917241\n","5                    LinearSVC         0  0.917241\n","34          LogisticRegression         4  0.915493\n","16  GradientBoostingClassifier         1  0.910345\n","15  GradientBoostingClassifier         0  0.910345\n","13                         SVC         3  0.909722\n","8                    LinearSVC         3  0.909722\n","19  GradientBoostingClassifier         4  0.908451\n","14                         SVC         4  0.908451\n","33          LogisticRegression         3  0.895833\n","26                  GaussianNB         1  0.882759\n","25                  GaussianNB         0  0.882759\n","20               MultinomialNB         0  0.868966\n","27                  GaussianNB         2  0.868056\n","29                  GaussianNB         4  0.852113\n","28                  GaussianNB         3  0.847222\n","23               MultinomialNB         3  0.847222\n","22               MultinomialNB         2  0.840278\n","24               MultinomialNB         4  0.802817\n","21               MultinomialNB         1  0.779310"]},"metadata":{"tags":[]},"execution_count":22}]},{"metadata":{"id":"fCh7wrc24rsD","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","filename = model_dir_path + 'lq_rf_model_1.sav'\n","rf = RandomForestClassifier(n_estimators=250, max_depth=4, random_state=0)\n","\n","rf.fit(train_data_2d, labels_train)\n","rf_pred = svc.predict(test_data_2d)\n","\n","pickle.dump(rf, open(filename, 'wb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZMFqQgfN7r3S","colab_type":"code","colab":{}},"cell_type":"code","source":["label_df = pd.DataFrame.from_dict(appliance_dict, orient='index')\n","label_list = list(label_df.index)\n","n_classes"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LTrSW7Dry9Fg","colab_type":"code","colab":{}},"cell_type":"code","source":["svc = SVC(C = 10, gamma=0.01, decision_function_shape='ovo')\n","\n","svc.fit(train_data_2d, labels_train)\n","svc_pred = svc.predict(test_data_2d)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J_PX2lZl7VyU","colab_type":"code","colab":{}},"cell_type":"code","source":["sns.set(font_scale=1.2)\n","conf_mat = confusion_matrix(labels_test, rf_pred)\n","fig, ax = plt.subplots(figsize=(12,10))\n","sns.heatmap(conf_mat, annot=True, fmt='d',\n","            xticklabels=label_list, yticklabels=label_list)\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.show()\n","\n","\n","print(\"Test Accuracy: %.3f%%\" % (accuracy_score(labels_test, rf)*100))\n","print(classification_report(labels_test, rf, target_names=label_list))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qS5mbtWW0Rnv","colab_type":"text"},"cell_type":"markdown","source":["------------------------------------------------------------------\n","# NEURAL NETWORK MODELS \n","------------------------------------------------------------------"]},{"metadata":{"id":"36UilelQcZWl","colab_type":"code","colab":{}},"cell_type":"code","source":["model = tf.keras.Sequential() # Use keras sequential layers to build up a model.\n","# model.add(tf.keras.layers.Flatten(input_shape=[50,8))\n","model.add(tf.keras.layers.Flatten(input_shape=[50,8,3]))\n","model.add(tf.keras.layers.Dense(num_labels*8, activation='relu'))\n","model.add(tf.keras.layers.Dense(num_labels*8, activation='relu'))\n","model.add(tf.keras.layers.Dense(num_labels*4, activation='relu'))\n","model.add(tf.keras.layers.Dense(num_labels, activation='softmax'))\n","\n","model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy','sparse_categorical_crossentropy'])\n","\n","model.summary()\n","print(\"\\n\\n\")\n","# Train the model, keras does not need datasets to function, can just take raw numpy inputs!\n","history = model.fit(data_train, \n","                    labels_train, \n","                    epochs=20,\n","                    batch_size=10,\n","                    validation_data=(data_test,labels_test),\n","                    callbacks=[cp_callback])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yRkIbxy2yhRc","colab_type":"code","colab":{}},"cell_type":"code","source":["# from random import randint\n","\n","# def warp_shrink(feat):\n","#     \"\"\"\n","#     Takes in a feature in the form of a numpy array, takes a random sample from the array and shrinks it by half.\n","#     The size of the random sample is equal to 1/10 of the size of the source array.  Replaces sample with pair-wise average of sample indices.\n","#     Approach inspired from this paper: https://aaltd16.irisa.fr/files/2016/08/AALTD16_paper_9.pdf\n","#     Parameters: feat - feature to be warped in form of numpy array\n","#     Returns: Feature array with sample shrank\n","#     \"\"\"\n","#     print('shape:',feat.shape)\n","#     #Get length of array\n","#     sample_range = feat.shape[0]\n","# #     print('sample_range',sample_range)\n","\n","#     #Calculate length of sample\n","#     sample_tenth = int(int(sample_range)*.1)\n","\n","#     #Shift sample length to be even to allow for pair-wise averages\n","#     if sample_tenth % 2 == 1:\n","#         sample_tenth += 1\n","\n","#     # Generate random index for start of sample\n","#     sample_index = randint(0, sample_range - (sample_tenth))\n","\n","#     #Shift sample index to be even to allow for pair-wise averages\n","#     if sample_index % 2 == 1:\n","#         sample_index += 1\n","\n","#     #Subset feature array into sample\n","#     sample = feat[sample_index:int(sample_index+sample_tenth)]\n","\n","#     avg_arr = []\n","#     for i in range(0, sample_tenth, 2): #iterate thru pairs of elements in sample, calculating average\n","#         avg = (sample[i]+sample[i+1])/2\n","#         avg_arr.append(avg)\n","\n","#     avg_arr = np.array(avg_arr)\n","\n","#     # Replace sample with shrunk average of samples\n","#     feat_before = feat[:sample_index]\n","#     feat_after = feat[int(sample_index+sample_tenth):]\n","#     feat_transformed = np.concatenate([feat_before,avg_arr,feat_after])\n","\n","#     return feat_transformed\n","\n","# def warp_stretch(feat):\n","#     \"\"\"\n","#     Takes in a feature in the form of a numpy array, takes a random sample from the array and shrinks it by half.\n","#     The size of the random sample is equal to 1/10 of the size of the source array.  Appends pair-wise average of every other sample index.\n","#     Approach inspired from this paper: https://aaltd16.irisa.fr/files/2016/08/AALTD16_paper_9.pdf\n","#     Parameters: feat - feature to be warped in form of numpy array\n","#     Returns: Feature array with sample stretched\n","#     \"\"\"\n","\n","#     # Get length of array\n","#     print('shape:',feat.shape)\n","#     sample_range = feat.shape[0]\n","# #     print('sample_range',sample_range)\n","\n","#     # Calculate length of sample\n","#     sample_tenth = int(int(sample_range)*.1)\n","\n","#     # Shift sample length to be even to allow for pair-wise averages\n","#     if sample_tenth % 2 == 1:\n","#         sample_tenth += 1\n","\n","#     # Generate random index for start of sample\n","#     sample_index = randint(0, sample_range - (sample_tenth))\n","\n","#     # Shift sample index to be even to allow for pair-wise averages\n","#     if sample_index % 2 == 1:\n","#         sample_index += 1\n","\n","#     # Subset feature array into sample\n","#     sample = feat[sample_index:int(sample_index + sample_tenth)]\n","\n","#     stretch_arr = []\n","#     for i in range(0, sample_tenth,2): #Iterate thru each pair of elements in the sample\n","#         if (i > 0 & i <= sample_tenth-1):  #Get average of previous index and current index\n","#             avg_prev = (sample[i-1]+sample[i])/2\n","#             stretch_arr.append(avg_prev)\n","#         avg = (sample[i]+sample[i+1])/2 #Get average of current index\n","#         stretch_arr.append(sample[i])\n","#         stretch_arr.append(avg)\n","#         stretch_arr.append(sample[i+1])\n","\n","#     stretch_arr = np.array(stretch_arr)\n","\n","#     #Replace sample with stretched sample\n","#     feat_before = feat[:sample_index]\n","#     feat_after = feat[int(sample_index+sample_tenth):]\n","#     feat_transformed = np.concatenate([feat_before,stretch_arr,feat_after])\n","\n","#     return feat_transformed\n","\n","\n","# def warp_features(observation):\n","#     \"\"\"\"\n","#     Applies warp shrink/stretch at random to numpy array of feature arrays.\n","#     Parameters: observation - numpy array of arrays\n","#     Returns: warped numpy array of arrays\n","#     \"\"\"\n","#     new_obs = []\n","#     for feature in range(0,observation.shape[1]): #Iterates thru each feature array\n","#         if (randint(0,1) > 0):\n","#             warped_feature = warp_stretch(observation[:,feature]) # applies stretch randomly\n","#         else:\n","#             warped_feature = warp_shrink(observation[:,feature]) # else applies shrink\n","#         new_obs.append(warped_feature)\n","#     np_array = np.asarray(new_obs)\n","#     return tf.convert_to_tensor(np_array)\n","\n","\n","# warped_data = warp_features(train_data)\n","# print(warped_data.shape)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sj1Ej2Gd3NVQ","colab_type":"code","colab":{}},"cell_type":"code","source":["#DATA AUGMENTATION\n","\n","# preprocessing_function: function that will be implied on each input. \n","#   The function will run after the image is resized and augmented. \n","#   The function should take one argument: one image (Numpy tensor with rank 3), \n","#     and should output a Numpy tensor with the same shape.\n","def apply_noise(array):#preprocessing_function...apply gaussian noise\n","#   return array\n","#   tensor = tf.convert_to_tensor(array)\n","#   shape = array.tf.get_shape()#.as_list()\n","#   print(type(tensor),tensor.get_shape().as_list())#,'shape',shape)\n","#   noise = np.random.normal(0,0.001,[50,8,3])#shape)#[144,50,8,1]) #or tensor.shape?\n","  return array #+ noise\n","  \n","train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=apply_noise)\n","# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=warp_features)\n","test_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n","\n","labels_train_np_1d = make3D(np.array(labels_train),1) #must have an additional channel for generator function\n","labels_test_np_1d = make3D(np.array(labels_test),1) #must have an additional channel for generator function\n","\n","print('train:',data_train.shape,type(data_train),'labels:',labels_train_np_1d.shape,type(labels_train_np_1d))\n","print('test:',data_test.shape,type(data_test),'labels:',labels_test_np_1d.shape,type(labels_test_np_1d))\n","\n","train_generator = train_datagen.flow(\n","#     train_data, #tensor w/ added axis #TypeError: unsupported operand type(s) for /: 'Dimension' and 'int'\n","#     train_dataset, #(Tensor dataset input): TypeError: object of type 'DatasetV1Adapter' has no len()\n","#     data_train, #(NP input) ValueError: ('Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape', (144, 50, 8))\n","    data_train, #(NP input) ValueError: ('Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape', (144, 50, 8))\n","    labels_train_np_1d,\n","#     train_labels,\n","# dataset_data_np, #ValueError: ('Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape', (161, 50, 8))\n","#       dataset_labels_int,\n","    batch_size=10)\n","\n","test_generator = test_datagen.flow(\n","    data_test, #(NP input required)\n","    labels_test_np_1d,\n","    batch_size=10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7T52K-ZBtaC2","colab_type":"code","colab":{}},"cell_type":"code","source":["aug_model = tf.keras.Sequential() # Use keras sequential layers to build up a model.\n","# aug_model.add(tf.keras.layers.Flatten(input_shape=[50,8]))\n","aug_model.add(tf.keras.layers.Flatten(input_shape=[50,8,3]))\n","aug_model.add(tf.keras.layers.Dense(num_labels*8, activation='relu'))\n","aug_model.add(tf.keras.layers.Dense(num_labels*8, activation='relu'))\n","aug_model.add(tf.keras.layers.Dense(num_labels*4, activation='relu'))\n","aug_model.add(tf.keras.layers.Dense(num_labels, activation='softmax'))\n","\n","aug_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy','sparse_categorical_crossentropy'])\n","aug_model.summary()\n","\n","print(\"\\n\\n\")\n","print('train:',data_train.shape,type(data_train),'labels:',labels_train_np_1d.shape,type(labels_train_np_1d))\n","print('test:',data_test.shape,type(data_test),'labels:',labels_test_np_1d.shape,type(labels_test_np_1d))\n","print(\"\\n\\n\")\n","\n","aug_history = aug_model.fit_generator(train_generator, validation_data=test_generator, epochs=20)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X_LoZcrYf6SB","colab_type":"code","colab":{}},"cell_type":"code","source":["louis_model = tf.keras.Sequential()\n","louis_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=[50,8,3]))\n","louis_model.add(tf.keras.layers.BatchNormalization())\n","louis_model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n","louis_model.add(tf.keras.layers.Dropout(0.3))\n","louis_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n","louis_model.add(tf.keras.layers.BatchNormalization())\n","louis_model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n","louis_model.add(tf.keras.layers.Dropout(0.3))\n","louis_model.add(tf.keras.layers.Flatten())\n","louis_model.add(tf.keras.layers.Dense(256, activation='relu'))\n","louis_model.add(tf.keras.layers.BatchNormalization())\n","louis_model.add(tf.keras.layers.Dropout(0.5))\n","louis_model.add(tf.keras.layers.Dense(7, activation='softmax'))\n","louis_model.add(tf.keras.layers.BatchNormalization())\n","louis_model.add(tf.keras.layers.Activation('softmax'))\n","\n","louis_model.summary()\n","print(\"\\n\\n\")\n",",\n","louis_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy','sparse_categorical_crossentropy'])\n","\n","louis_history = model.fit(data_train, labels_train, epochs=20,batch_size=10,validation_data=(data_test,labels_test))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tIFAOlW0Fll-","colab_type":"text"},"cell_type":"markdown","source":["------------------------------------------------------------------\n","# TESTING MODELS\n","------------------------------------------------------------------"]},{"metadata":{"id":"icCw2u4l6-g5","colab_type":"code","colab":{}},"cell_type":"code","source":["# def plot_history(histories, key='binary_crossentropy'):\n","def plot_history(histories, key='sparse_categorical_crossentropy'):\n","  \n","  plt.figure(figsize=(16,10))\n","    \n","  for name, history in histories:\n","    val = plt.plot(history.epoch, history.history['val_'+key],'--', label=name.title()+' Val')\n","    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),label=name.title()+' Train')\n","\n","  plt.xlabel('Epochs')\n","  plt.ylabel(key.replace('_',' ').title())\n","  plt.legend()\n","\n","  plt.xlim([0,max(history.epoch)])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xf1DHEOF6ZGY","colab_type":"code","colab":{}},"cell_type":"code","source":["plot_history([('baseline', history), ('augmented', aug_history),('louis',louis_history)], key='acc')\n","plot_history([('baseline', history), ('augmented', aug_history),('louis',louis_history)], key='sparse_categorical_crossentropy')\n","plot_history([('baseline', history), ('augmented', aug_history),('louis',louis_history)], key='loss')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZEyGu1hk8dfL","colab_type":"code","colab":{}},"cell_type":"code","source":["#{'hairdryer':0,'laptop':1,'inductioncooktop':2,'cell':3,'heatpad':4,'LEDpicture':5,'kettle':6}\n","\n","# # Evaluate on test data after training.\n","# test_loss, test_acc = model.evaluate(data_test, labels_test)\n","# # test_loss, test_acc = model.evaluate(test_data, test_labels)\n","# print(\"Test Accuracy:\", test_acc,'\\n')\n","\n","# Can also make predictions on individual samples.\n","\n","inv_map = {v: k for k, v in appliance_dict.items()}\n","\n","for i,data in enumerate(data_test):\n","  data = data[tf.newaxis, ...]\n","  print('data.shape',data.shape)\n","  guess_list = model.predict(data)\n","  print(guess_list)\n","#   guess = tf.argmax(guess_list, axis=-1).numpy()\n","#   topk = tf.nn.top_k(model.predict(data),k=3)#.numpy()\n","#   print(topk)\n","#   print('guess:',guess,convert_int_to_str(guess,appliance_dict))\n","  print('actual:',labels_test[i],convert_int_to_str(labels_test[i],appliance_dict))\n","#   print(guesses)\n","  for j in range (0,7):\n","    print(('guesses: %.2f%% %s'%(100*guess_list[0,j],inv_map[j])) )\n","  print('')\n","\n","  guess_list.argmax"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-rZiaDf4gdrK","colab_type":"code","colab":{}},"cell_type":"code","source":["guess_list."],"execution_count":0,"outputs":[]},{"metadata":{"id":"H3U6Idt6GgdD","colab_type":"code","colab":{}},"cell_type":"code","source":["print(inv_map)\n","print(appliance_dict)\n","\n","print(os.listdir(example_path))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9EfjOyczGnZa","colab_type":"code","colab":{}},"cell_type":"code","source":["# Let’s go ahead and load the model, this would be what we do when starting a new notebook.\n","model = tf.keras.models.load_model(checkpoint_path)\n","# Evaluate to make sure the accuracy is preserved.\n","model.evaluate(data_test, labels_test)"],"execution_count":0,"outputs":[]}]}